from bs4 import BeautifulSoup
from urllib.request import urlopen 
import urllib.request
import requests

#==============================================================================#

used = dict()
urls = []

with open('urls.txt', 'r') as url_file:
    for line in url_file.readlines():
        urls.append(line.strip())       

url_file = open('urls.txt', 'a')

for url in urls:
    used[url] = True

def getText(soup):    
    for script in soup(["script", "style"]):
         script.extract() 
   
    text = soup.get_text()

    lines = (line.strip() for line in text.splitlines())   
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    
    text = '\n'.join(chunk for chunk in chunks if chunk)
    
   # print(text)

#===============================================================================#

def getData(URL, depth):

    url_file.write(URL)
    
    if depth == 2:
        return
    
    used[URL] = True

    html_page = urllib.request.urlopen(URL)
    soup = BeautifulSoup(html_page, "html.parser")
    getText(soup)

    for link in soup.findAll('a'): 
        URL = link.get('href')
        if URL and not URL.startswith('#'):
            if not URL.startswith(DOMAIN):
                URL = DOMAIN + URL
            if used.get(URL):
                continue
            getData(URL, depth + 1)  

#==============================================================================#

DOMAIN = "https://en.wikipedia.org"
START_URL = f"{DOMAIN}/wiki/Main_Page"
getData(START_URL, 0)

